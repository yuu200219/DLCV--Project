{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detectron2\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_test_loader\n",
    "from detectron2.utils.visualizer import ColorMode, Visualizer\n",
    "from detectron2.evaluation import PascalVOCDetectionEvaluator, inference_on_dataset\n",
    "import os, json, cv2, random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Dataset 'my_dataset' is already registered!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdetectron2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_pascal_voc\n\u001b[1;32m      3\u001b[0m cls_names \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruck\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcar\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrider\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperson\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmotorcycle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbicycle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbus\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mregister_pascal_voc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy_dataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/mnt/c/Users/user/OneDrive - 國立中正大學/lesson/112-2/ML_CV/Exercise_1/datasets/Cityscapes_dataset/Cityscapes_dataset/VOC2007\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrainval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2007\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcls_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m register_pascal_voc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_test\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/c/Users/user/OneDrive - 國立中正大學/lesson/112-2/ML_CV/Exercise_1/datasets/Cityscapes_dataset/Cityscapes_dataset/VOC2007\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2007\u001b[39m, cls_names)\n",
      "File \u001b[0;32m~/Sources/detectron2/detectron2/data/datasets/pascal_voc.py:79\u001b[0m, in \u001b[0;36mregister_pascal_voc\u001b[0;34m(name, dirname, split, year, class_names)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mregister_pascal_voc\u001b[39m(name, dirname, split, year, class_names\u001b[38;5;241m=\u001b[39mCLASS_NAMES):\n\u001b[0;32m---> 79\u001b[0m     \u001b[43mDatasetCatalog\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_voc_instances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     MetadataCatalog\u001b[38;5;241m.\u001b[39mget(name)\u001b[38;5;241m.\u001b[39mset(\n\u001b[1;32m     81\u001b[0m         thing_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(class_names), dirname\u001b[38;5;241m=\u001b[39mdirname, year\u001b[38;5;241m=\u001b[39myear, split\u001b[38;5;241m=\u001b[39msplit\n\u001b[1;32m     82\u001b[0m     )\n",
      "File \u001b[0;32m~/Sources/detectron2/detectron2/data/catalog.py:37\u001b[0m, in \u001b[0;36m_DatasetCatalog.register\u001b[0;34m(self, name, func)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    name (str): the name that identifies a dataset, e.g. \"coco_2014_train\".\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    func (callable): a callable which takes no arguments and returns a list of dicts.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m        It must return the same results if called multiple times.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(func), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must register a function with `DatasetCatalog.register`!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is already registered!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m[name] \u001b[38;5;241m=\u001b[39m func\n",
      "\u001b[0;31mAssertionError\u001b[0m: Dataset 'my_dataset' is already registered!"
     ]
    }
   ],
   "source": [
    "from detectron2.data.datasets import register_pascal_voc\n",
    "\n",
    "cls_names = ('truck', 'car', 'rider', 'person', 'train', 'motorcycle', 'bicycle', 'bus')\n",
    "register_pascal_voc(\"my_dataset\", '/mnt/c/Users/user/OneDrive - 國立中正大學/lesson/112-2/ML_CV/Exercise_1/datasets/Cityscapes_dataset/Cityscapes_dataset/VOC2007', \"trainval\", 2007, cls_names)\n",
    "register_pascal_voc(\"my_test\", '/mnt/c/Users/user/OneDrive - 國立中正大學/lesson/112-2/ML_CV/Exercise_1/datasets/Cityscapes_dataset/Cityscapes_dataset/VOC2007', \"test\", 2007, cls_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Non-existent config key: MODEL.FREEZE_AT'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdetectron2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DetectionCheckpointer\n\u001b[1;32m      4\u001b[0m cfg \u001b[38;5;241m=\u001b[39m get_cfg()\n\u001b[0;32m----> 5\u001b[0m \u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./PascalVOC-Detection/my_vgg.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m cfg\u001b[38;5;241m.\u001b[39mDATASETS\u001b[38;5;241m.\u001b[39mTRAIN \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m,)\n\u001b[1;32m      7\u001b[0m cfg\u001b[38;5;241m.\u001b[39mDATASETS\u001b[38;5;241m.\u001b[39mTEST \u001b[38;5;241m=\u001b[39m ()\n",
      "File \u001b[0;32m~/Sources/detectron2/detectron2/config/config.py:69\u001b[0m, in \u001b[0;36mCfgNode.merge_from_file\u001b[0;34m(self, cfg_filename, allow_unsafe)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m loaded_ver \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mVERSION, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot merge a v\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m config into a v\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m config.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     65\u001b[0m     loaded_ver, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mVERSION\n\u001b[1;32m     66\u001b[0m )\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loaded_ver \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mVERSION:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_from_other_cfg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaded_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# compat.py needs to import CfgNode\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m upgrade_config, downgrade_config\n",
      "File \u001b[0;32m~/anaconda3/envs/detectron2/lib/python3.12/site-packages/fvcore/common/config.py:132\u001b[0m, in \u001b[0;36mCfgNode.merge_from_other_cfg\u001b[0;34m(self, cfg_other)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    cfg_other (CfgNode): configs to merge from.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    130\u001b[0m     BASE_KEY \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m cfg_other\n\u001b[1;32m    131\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe reserved key \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m can only be used in files!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(BASE_KEY)\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_from_other_cfg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg_other\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/detectron2/lib/python3.12/site-packages/yacs/config.py:217\u001b[0m, in \u001b[0;36mCfgNode.merge_from_other_cfg\u001b[0;34m(self, cfg_other)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge_from_other_cfg\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg_other):\n\u001b[1;32m    216\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Merge `cfg_other` into this CfgNode.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m     \u001b[43m_merge_a_into_b\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg_other\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/detectron2/lib/python3.12/site-packages/yacs/config.py:478\u001b[0m, in \u001b[0;36m_merge_a_into_b\u001b[0;34m(a, b, root, key_list)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, CfgNode):\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 478\u001b[0m         \u001b[43m_merge_a_into_b\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_list\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/detectron2/lib/python3.12/site-packages/yacs/config.py:491\u001b[0m, in \u001b[0;36m_merge_a_into_b\u001b[0;34m(a, b, root, key_list)\u001b[0m\n\u001b[1;32m    489\u001b[0m     root\u001b[38;5;241m.\u001b[39mraise_key_rename_error(full_key)\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon-existent config key: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(full_key))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Non-existent config key: MODEL.FREEZE_AT'"
     ]
    }
   ],
   "source": [
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(\"./PascalVOC-Detection/my_vgg.yaml\")\n",
    "cfg.DATASETS.TRAIN = (\"my_dataset\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 1\n",
    "# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"PascalVOC-Detection/faster_rcnn_R_50_FPN.yaml\")  # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2 # This is the real \"batch size\" commonly known to deep learning people\n",
    "cfg.SOLVER.BASE_LR = 0.0002  # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = 90000    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
    "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # The \"RoIHead batch size\". 128 is faster, and good enough for this toy dataset (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 8  # 8 classes, class_names = ('truck', 'car', 'rider', 'person', 'train', 'motorcycle', 'bicycle', 'bus')\n",
    "\n",
    "cfg.MODEL.BACKBONE.FREEZE_AT = 2\n",
    "#cfg.MODEL.WEIGHTS = \"./output/vgg16/model_0064999.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/yuu/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myuu_910119\u001b[0m (\u001b[33m112-2_ml_cv\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/yuu/Sources/detectron2/configs/wandb/run-20240407_164247-uq1no1px</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/112-2_ml_cv/Exercise_1/runs/uq1no1px/workspace' target=\"_blank\">VGG16-v2</a></strong> to <a href='https://wandb.ai/112-2_ml_cv/Exercise_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/112-2_ml_cv/Exercise_1' target=\"_blank\">https://wandb.ai/112-2_ml_cv/Exercise_1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/112-2_ml_cv/Exercise_1/runs/uq1no1px/workspace' target=\"_blank\">https://wandb.ai/112-2_ml_cv/Exercise_1/runs/uq1no1px/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/112-2_ml_cv/Exercise_1/runs/uq1no1px?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7feff7da7800>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(relogin=True, key='78f66d0691441fbb503f17c6de791883d0e54f94')\n",
    "wandb.init(\n",
    "            # set the wandb project where this run will be logged\n",
    "            project=\"Exercise_1\", \n",
    "            name=\"VGG16-v2\",\n",
    "            notes=\"20240407_v2\",\n",
    "            sync_tensorboard=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/07 16:43:05 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): VGG(\n",
      "    (vgg_block1): Sequential(\n",
      "      (0): VGGBlock(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (vgg_block2): Sequential(\n",
      "      (0): VGGBlock(\n",
      "        (conv1): Conv2d(\n",
      "          64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "        )\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (vgg_block3): Sequential(\n",
      "      (0): VGGBlock(\n",
      "        (conv1): Conv2d(\n",
      "          128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "        )\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (vgg_block4): Sequential(\n",
      "      (0): VGGBlock(\n",
      "        (conv1): Conv2d(\n",
      "          256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (vgg_block5): Sequential(\n",
      "      (0): VGGBlock(\n",
      "        (conv1): Conv2d(\n",
      "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv2): Conv2d(\n",
      "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "        (conv3): Conv2d(\n",
      "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(512, 9, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(512, 36, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): VggROIHeads(\n",
      "    (pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=512, out_features=9, bias=True)\n",
      "      (bbox_pred): Linear(in_features=512, out_features=32, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[04/07 16:43:47 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 2965 images left.\n",
      "\u001b[32m[04/07 16:43:47 d2.data.build]: \u001b[0mDistribution of instances among all 8 categories:\n",
      "\u001b[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |\n",
      "|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
      "|   truck    | 489          |    car     | 27155        |   rider    | 1807         |\n",
      "|   person   | 17994        |   train    | 171          | motorcycle | 739          |\n",
      "|  bicycle   | 3729         |    bus     | 385          |            |              |\n",
      "|   total    | 52469        |            |              |            |              |\u001b[0m\n",
      "\u001b[32m[04/07 16:43:47 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[04/07 16:43:47 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[04/07 16:43:47 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[04/07 16:43:47 d2.data.common]: \u001b[0mSerializing 2965 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[04/07 16:43:47 d2.data.common]: \u001b[0mSerialized dataset takes 4.29 MiB\n",
      "\u001b[32m[04/07 16:43:47 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=2\n",
      "\u001b[32m[04/07 16:43:47 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from  ...\n",
      "\u001b[32m[04/07 16:43:47 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuu/anaconda3/envs/detectron2/lib/python3.12/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/07 16:44:05 d2.utils.events]: \u001b[0m eta: 0:01:03  iter: 19  total_loss: 3.232  loss_cls: 1.213  loss_box_reg: 0.2837  loss_rpn_cls: 0.6456  loss_rpn_loc: 0.8565    time: 0.8029  last_time: 0.7883  data_time: 0.0287  last_data_time: 0.0062   lr: 3.8162e-05  max_mem: 1537M\n",
      "\u001b[32m[04/07 16:44:21 d2.utils.events]: \u001b[0m eta: 0:00:48  iter: 39  total_loss: 2.179  loss_cls: 0.7698  loss_box_reg: 0.196  loss_rpn_cls: 0.5921  loss_rpn_loc: 0.5639    time: 0.8047  last_time: 0.8268  data_time: 0.0061  last_data_time: 0.0063   lr: 7.8122e-05  max_mem: 1537M\n",
      "\u001b[32m[04/07 16:44:37 d2.utils.events]: \u001b[0m eta: 0:00:32  iter: 59  total_loss: 2.063  loss_cls: 0.6909  loss_box_reg: 0.2571  loss_rpn_cls: 0.4992  loss_rpn_loc: 0.6901    time: 0.8103  last_time: 0.8424  data_time: 0.0065  last_data_time: 0.0050   lr: 0.00011808  max_mem: 1537M\n",
      "\u001b[32m[04/07 16:44:54 d2.utils.events]: \u001b[0m eta: 0:00:16  iter: 79  total_loss: 2.052  loss_cls: 0.6938  loss_box_reg: 0.2214  loss_rpn_cls: 0.4832  loss_rpn_loc: 0.5406    time: 0.8155  last_time: 0.8400  data_time: 0.0066  last_data_time: 0.0063   lr: 0.00015804  max_mem: 1537M\n",
      "\u001b[32m[04/07 16:44:59 d2.engine.hooks]: \u001b[0mOverall training speed: 83 iterations in 0:01:08 (0.8267 s / it)\n",
      "\u001b[32m[04/07 16:44:59 d2.engine.hooks]: \u001b[0mTotal training time: 0:01:08 (0:00:00 on hooks)\n",
      "\u001b[32m[04/07 16:44:59 d2.utils.events]: \u001b[0m eta: 0:00:11  iter: 85  total_loss: 2.142  loss_cls: 0.7352  loss_box_reg: 0.3251  loss_rpn_cls: 0.4638  loss_rpn_loc: 0.5895    time: 0.8167  last_time: 0.8430  data_time: 0.0068  last_data_time: 0.0074   lr: 0.00016803  max_mem: 1537M\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m DefaultTrainer(cfg)\n\u001b[1;32m      4\u001b[0m trainer\u001b[38;5;241m.\u001b[39mresume_or_load(resume\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Sources/detectron2/detectron2/engine/defaults.py:486\u001b[0m, in \u001b[0;36mDefaultTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    480\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m    Run training.\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \n\u001b[1;32m    483\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;124;03m        OrderedDict of results, if evaluation is enabled. Otherwise None.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 486\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mTEST\u001b[38;5;241m.\u001b[39mEXPECTED_RESULTS) \u001b[38;5;129;01mand\u001b[39;00m comm\u001b[38;5;241m.\u001b[39mis_main_process():\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_last_eval_results\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    490\u001b[0m         ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo evaluation results obtained during training!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Sources/detectron2/detectron2/engine/train_loop.py:155\u001b[0m, in \u001b[0;36mTrainerBase.train\u001b[0;34m(self, start_iter, max_iter)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_iter, max_iter):\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbefore_step()\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter_step()\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# self.iter == max_iter can be used by `after_train` to\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# tell whether the training successfully finished or failed\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# due to exceptions.\u001b[39;00m\n",
      "File \u001b[0;32m~/Sources/detectron2/detectron2/engine/defaults.py:496\u001b[0m, in \u001b[0;36mDefaultTrainer.run_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39miter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter\n\u001b[0;32m--> 496\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Sources/detectron2/detectron2/engine/train_loop.py:332\u001b[0m, in \u001b[0;36mSimpleTrainer.run_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcurrent_executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_metrics, loss_dict, data_time, \u001b[38;5;28miter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter\n\u001b[1;32m    330\u001b[0m     )\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03mIf you need gradient clipping/scaling or other processing, you can\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03mwrap the optimizer with your custom `step()` method. But it is\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03msuboptimal as explained in https://arxiv.org/abs/2006.15704 Sec 3.2.4\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Sources/detectron2/detectron2/engine/train_loop.py:370\u001b[0m, in \u001b[0;36mSimpleTrainer._write_metrics\u001b[0;34m(self, loss_dict, data_time, prefix, iter)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28miter\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_metric_period \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 370\u001b[0m         \u001b[43mSimpleTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    372\u001b[0m         logger\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException in writing metrics: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Sources/detectron2/detectron2/engine/train_loop.py:388\u001b[0m, in \u001b[0;36mSimpleTrainer.write_metrics\u001b[0;34m(loss_dict, data_time, cur_iter, prefix)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite_metrics\u001b[39m(\n\u001b[1;32m    377\u001b[0m     loss_dict: Mapping[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    380\u001b[0m     prefix: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    381\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    382\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;124;03m        loss_dict (dict): dict of scalar losses\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;124;03m        data_time (float): time taken by the dataloader iteration\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m        prefix (str): prefix for logging keys\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 388\u001b[0m     metrics_dict \u001b[38;5;241m=\u001b[39m {k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    389\u001b[0m     metrics_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_time\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data_time\n\u001b[1;32m    391\u001b[0m     storage \u001b[38;5;241m=\u001b[39m get_event_storage()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cfg.OUTPUT_DIR='./output/vgg16/ver2'\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/07 20:51:09 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from ./output/vgg16/again/model_final.pth ...\n"
     ]
    }
   ],
   "source": [
    "cfg.OUTPUT_DIR='./output/vgg16/again'\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuu/anaconda3/envs/detectron2/lib/python3.12/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "# INFERENCE\n",
    "# ----------\n",
    "# VISUALIZE TEH PREDICT RESULT\n",
    "im = cv2.imread('aachen_000012_000019_leftImg8bit.png')\n",
    "outputs = predictor(im)\n",
    "v = Visualizer(im[:, :, ::-1],\n",
    "                metadata=MetadataCatalog.get('my_test'),\n",
    "                scale=0.5,\n",
    "                instance_mode=ColorMode.IMAGE_BW\n",
    ")\n",
    "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "cv2.imshow('', out.get_image()[:, :, ::-1])\n",
    "cv2.waitKey(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/07 20:51:30 d2.data.build]: \u001b[0mDistribution of instances among all 8 categories:\n",
      "\u001b[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |\n",
      "|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
      "|   truck    | 93           |    car     | 4667         |   rider    | 556          |\n",
      "|   person   | 3419         |   train    | 23           | motorcycle | 149          |\n",
      "|  bicycle   | 1175         |    bus     | 98           |            |              |\n",
      "|   total    | 10180        |            |              |            |              |\u001b[0m\n",
      "\u001b[32m[04/07 20:51:30 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[04/07 20:51:30 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[04/07 20:51:30 d2.data.common]: \u001b[0mSerializing 492 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[04/07 20:51:30 d2.data.common]: \u001b[0mSerialized dataset takes 0.79 MiB\n",
      "\u001b[32m[04/07 20:51:30 d2.evaluation.evaluator]: \u001b[0mStart inference on 492 batches\n",
      "\u001b[32m[04/07 20:51:36 d2.evaluation.evaluator]: \u001b[0mInference done 11/492. Dataloading: 0.0029 s/iter. Inference: 0.2080 s/iter. Eval: 0.0007 s/iter. Total: 0.2116 s/iter. ETA=0:01:41\n",
      "\u001b[32m[04/07 20:51:41 d2.evaluation.evaluator]: \u001b[0mInference done 35/492. Dataloading: 0.0018 s/iter. Inference: 0.2077 s/iter. Eval: 0.0006 s/iter. Total: 0.2103 s/iter. ETA=0:01:36\n",
      "\u001b[32m[04/07 20:51:46 d2.evaluation.evaluator]: \u001b[0mInference done 59/492. Dataloading: 0.0017 s/iter. Inference: 0.2085 s/iter. Eval: 0.0006 s/iter. Total: 0.2110 s/iter. ETA=0:01:31\n",
      "\u001b[32m[04/07 20:51:52 d2.evaluation.evaluator]: \u001b[0mInference done 83/492. Dataloading: 0.0018 s/iter. Inference: 0.2108 s/iter. Eval: 0.0007 s/iter. Total: 0.2134 s/iter. ETA=0:01:27\n",
      "\u001b[32m[04/07 20:51:57 d2.evaluation.evaluator]: \u001b[0mInference done 106/492. Dataloading: 0.0020 s/iter. Inference: 0.2130 s/iter. Eval: 0.0007 s/iter. Total: 0.2158 s/iter. ETA=0:01:23\n",
      "\u001b[32m[04/07 20:52:02 d2.evaluation.evaluator]: \u001b[0mInference done 129/492. Dataloading: 0.0019 s/iter. Inference: 0.2138 s/iter. Eval: 0.0007 s/iter. Total: 0.2166 s/iter. ETA=0:01:18\n",
      "\u001b[32m[04/07 20:52:07 d2.evaluation.evaluator]: \u001b[0mInference done 151/492. Dataloading: 0.0021 s/iter. Inference: 0.2159 s/iter. Eval: 0.0008 s/iter. Total: 0.2190 s/iter. ETA=0:01:14\n",
      "\u001b[32m[04/07 20:52:12 d2.evaluation.evaluator]: \u001b[0mInference done 173/492. Dataloading: 0.0026 s/iter. Inference: 0.2167 s/iter. Eval: 0.0008 s/iter. Total: 0.2203 s/iter. ETA=0:01:10\n",
      "\u001b[32m[04/07 20:52:17 d2.evaluation.evaluator]: \u001b[0mInference done 196/492. Dataloading: 0.0026 s/iter. Inference: 0.2172 s/iter. Eval: 0.0008 s/iter. Total: 0.2207 s/iter. ETA=0:01:05\n",
      "\u001b[32m[04/07 20:52:22 d2.evaluation.evaluator]: \u001b[0mInference done 219/492. Dataloading: 0.0025 s/iter. Inference: 0.2170 s/iter. Eval: 0.0008 s/iter. Total: 0.2205 s/iter. ETA=0:01:00\n",
      "\u001b[32m[04/07 20:52:27 d2.evaluation.evaluator]: \u001b[0mInference done 242/492. Dataloading: 0.0024 s/iter. Inference: 0.2172 s/iter. Eval: 0.0008 s/iter. Total: 0.2206 s/iter. ETA=0:00:55\n",
      "\u001b[32m[04/07 20:52:33 d2.evaluation.evaluator]: \u001b[0mInference done 265/492. Dataloading: 0.0024 s/iter. Inference: 0.2177 s/iter. Eval: 0.0008 s/iter. Total: 0.2210 s/iter. ETA=0:00:50\n",
      "\u001b[32m[04/07 20:52:38 d2.evaluation.evaluator]: \u001b[0mInference done 288/492. Dataloading: 0.0024 s/iter. Inference: 0.2179 s/iter. Eval: 0.0009 s/iter. Total: 0.2213 s/iter. ETA=0:00:45\n",
      "\u001b[32m[04/07 20:52:43 d2.evaluation.evaluator]: \u001b[0mInference done 311/492. Dataloading: 0.0024 s/iter. Inference: 0.2179 s/iter. Eval: 0.0009 s/iter. Total: 0.2212 s/iter. ETA=0:00:40\n",
      "\u001b[32m[04/07 20:52:48 d2.evaluation.evaluator]: \u001b[0mInference done 334/492. Dataloading: 0.0023 s/iter. Inference: 0.2177 s/iter. Eval: 0.0008 s/iter. Total: 0.2210 s/iter. ETA=0:00:34\n",
      "\u001b[32m[04/07 20:52:53 d2.evaluation.evaluator]: \u001b[0mInference done 357/492. Dataloading: 0.0023 s/iter. Inference: 0.2176 s/iter. Eval: 0.0008 s/iter. Total: 0.2208 s/iter. ETA=0:00:29\n",
      "\u001b[32m[04/07 20:52:58 d2.evaluation.evaluator]: \u001b[0mInference done 380/492. Dataloading: 0.0023 s/iter. Inference: 0.2176 s/iter. Eval: 0.0008 s/iter. Total: 0.2208 s/iter. ETA=0:00:24\n",
      "\u001b[32m[04/07 20:53:03 d2.evaluation.evaluator]: \u001b[0mInference done 403/492. Dataloading: 0.0022 s/iter. Inference: 0.2176 s/iter. Eval: 0.0008 s/iter. Total: 0.2208 s/iter. ETA=0:00:19\n",
      "\u001b[32m[04/07 20:53:08 d2.evaluation.evaluator]: \u001b[0mInference done 426/492. Dataloading: 0.0022 s/iter. Inference: 0.2177 s/iter. Eval: 0.0008 s/iter. Total: 0.2209 s/iter. ETA=0:00:14\n",
      "\u001b[32m[04/07 20:53:13 d2.evaluation.evaluator]: \u001b[0mInference done 449/492. Dataloading: 0.0022 s/iter. Inference: 0.2179 s/iter. Eval: 0.0008 s/iter. Total: 0.2210 s/iter. ETA=0:00:09\n",
      "\u001b[32m[04/07 20:53:18 d2.evaluation.evaluator]: \u001b[0mInference done 472/492. Dataloading: 0.0022 s/iter. Inference: 0.2181 s/iter. Eval: 0.0008 s/iter. Total: 0.2211 s/iter. ETA=0:00:04\n",
      "\u001b[32m[04/07 20:53:23 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:01:47.780238 (0.221315 s / iter per device, on 1 devices)\n",
      "\u001b[32m[04/07 20:53:23 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:46 (0.218190 s / iter per device, on 1 devices)\n",
      "\u001b[32m[04/07 20:53:23 d2.evaluation.pascal_voc_evaluation]: \u001b[0mEvaluating my_test using 2007 metric. Note that results do not use the official Matlab API.\n",
      "truck\n",
      "AP: 0.09090909090909091\n",
      "AP50: 4.545454545454546\n",
      "AP75: 0.0\n",
      "car\n",
      "AP: 1.8078246161145173\n",
      "AP50: 25.02996680051936\n",
      "AP75: 4.004103925568752\n",
      "rider\n",
      "AP: 1.019639333625711\n",
      "AP50: 13.88888888888889\n",
      "AP75: 2.4242424242424243\n",
      "person\n",
      "AP: 0.6655155471181206\n",
      "AP50: 11.317384386681901\n",
      "AP75: 1.0356731875719216\n",
      "train\n",
      "AP: 0.0\n",
      "AP50: 0.0\n",
      "AP75: 0.0\n",
      "motorcycle\n",
      "AP: 0.5454545454545455\n",
      "AP50: 4.545454545454546\n",
      "AP75: 4.545454545454546\n",
      "bicycle\n",
      "AP: 0.22623009179818557\n",
      "AP50: 3.54913058699965\n",
      "AP75: 0.6993006993006993\n",
      "bus\n",
      "AP: 0.6060606060606061\n",
      "AP50: 9.090909090909092\n",
      "AP75: 0.7575757575757575\n",
      "OrderedDict({'bbox': {'AP': 3.101021144425486, 'AP50': 8.9958986056135, 'AP75': 1.6832938174642627}})\n"
     ]
    }
   ],
   "source": [
    "# -------\n",
    "# EVALUATE\n",
    "# --------\n",
    "evaluator = PascalVOCDetectionEvaluator(\"my_test\")\n",
    "val_loader = build_detection_test_loader(cfg, \"my_test\")\n",
    "print(inference_on_dataset(predictor.model, val_loader, evaluator))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
